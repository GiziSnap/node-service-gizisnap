{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ6qy1xfxndd"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Instalasi Library yang Dibutuhkan\n",
        "!pip install tensorflow==2.15.0 tf-keras==2.15.1 keras==2.15.0 numpy==1.26.4 protobuf==4.25.7 h5py==3.13.0 pandas==2.2.2 scikit-learn==1.6.1 keras-tuner==1.4.7 matplotlib==3.10.3 seaborn==0.13.2 tensorflowjs==4.22.0 kagglehub==0.3.12 split-folders==0.5.1 tensorflow-text==2.15.0 dopamine-rl==4.0.7 thinc==8.2.3 grpcio-status==1.59.0 grpcio==1.59.0 packaging==23.2 tensorflow-decision-forests==1.8.1 spacy==3.7.5 google-cloud-bigquery==3.17.0 ml-dtypes==0.2.0 wrapt==1.14.1 tensorboard==2.15.2 tensorflow-estimator==2.15.0 --upgrade --no-cache-dir\n",
        "print(\"Instalasi library selesai.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Imports and Setup with Updated Authentication\n",
        "# Import necessary libraries (yang belum di-import di sel instalasi atau yang spesifik untuk blok ini)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import joblib # For saving scalers\n",
        "import shutil # For zipping files\n",
        "\n",
        "# For Google Colab integration\n",
        "from google.colab import auth, files\n",
        "import gspread\n",
        "from google.auth import default # Updated import for gspread authentication\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Authenticate user\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Get default credentials and authorize gspread\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "print(\"‚úÖ Libraries imported and Google Sheets authorized using google-auth.\")"
      ],
      "metadata": {
        "id": "dRFZmw5LxzH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Load Data Configuration and Execution\n",
        "\n",
        "# --- USER INPUT REQUIRED (Updated for data from scraping script) ---\n",
        "#@markdown Enter the name of your Google Sheet (output from scraping script):\n",
        "GOOGLE_SHEET_NAME = \"Scrap_Food_Datasets\" #@param {type:\"string\"}\n",
        "#@markdown Enter the name of the worksheet containing the scraped nutrition data:\n",
        "WORKSHEET_NAME = \"Result_Food_Name_List\" #@param {type:\"string\"}\n",
        "# --- END USER INPUT ---\n",
        "\n",
        "def load_data_from_google_sheet(sheet_name, worksheet_name):\n",
        "    \"\"\"Loads data from a Google Sheet into a pandas DataFrame.\"\"\"\n",
        "    try:\n",
        "        print(f\"Attempting to open spreadsheet: '{sheet_name}', worksheet: '{worksheet_name}'\")\n",
        "        spreadsheet_obj = gc.open(sheet_name) # gc should be defined from the previous cell\n",
        "        worksheet = spreadsheet_obj.worksheet(worksheet_name)\n",
        "        data = worksheet.get_all_values()\n",
        "        if not data:\n",
        "            print(f\"‚ùå Worksheet '{worksheet_name}' is empty or data could not be fetched.\")\n",
        "            return pd.DataFrame()\n",
        "        headers = data.pop(0)\n",
        "        df = pd.DataFrame(data, columns=headers)\n",
        "        df.columns = [col.strip() for col in df.columns] # Clean column names\n",
        "        print(f\"‚úÖ Data loaded successfully from Google Sheet: '{sheet_name}' (Worksheet: '{worksheet_name}')\")\n",
        "        return df\n",
        "    except gspread.exceptions.SpreadsheetNotFound:\n",
        "        print(f\"‚ùå Error: Spreadsheet '{sheet_name}' not found. Please check the name and sharing permissions.\")\n",
        "        return pd.DataFrame()\n",
        "    except gspread.exceptions.WorksheetNotFound:\n",
        "        print(f\"‚ùå Error: Worksheet '{worksheet_name}' not found in Spreadsheet '{sheet_name}'.\")\n",
        "        return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading data from Google Sheet: {e}\")\n",
        "        print(\"‚ö†Ô∏è Please ensure the Google Sheet name and worksheet name are correct, and that the sheet is shared appropriately.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Load the raw data\n",
        "df_raw = load_data_from_google_sheet(GOOGLE_SHEET_NAME, WORKSHEET_NAME)\n",
        "\n",
        "if not df_raw.empty:\n",
        "    print(\"\\nüìã Sample of loaded data (df_raw):\")\n",
        "    print(df_raw.head())\n",
        "    print(f\"\\nShape of loaded data: {df_raw.shape}\")\n",
        "    print(f\"\\nColumns in loaded data: {df_raw.columns.tolist()}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Data loading failed. Subsequent cells may not work correctly.\")"
      ],
      "metadata": {
        "id": "H8pnego8ZoS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Define Data Preprocessing Functions (NaN in Nutrition Columns to 0) (MODIFIED)\n",
        "# Helper functions to extract region, availability and origin information from the 'label' column\n",
        "def extract_region(label_text):\n",
        "    if pd.isna(label_text):\n",
        "        return \"Unknown\"\n",
        "    region_match = re.search(r'Region: (.*?)($|;|,)', str(label_text))\n",
        "    if region_match:\n",
        "        return region_match.group(1).strip()\n",
        "    if \"General\" in str(label_text) or \"Umum di:\" in str(label_text):\n",
        "        return \"General\"\n",
        "    return \"Unknown\"\n",
        "\n",
        "def extract_availability(label_text):\n",
        "    if pd.isna(label_text):\n",
        "        return \"Unknown\"\n",
        "    if \"General\" in str(label_text):\n",
        "        return \"General\"\n",
        "    availability_match = re.search(r'Umum di: (.*?)($|;|,)', str(label_text))\n",
        "    if availability_match:\n",
        "        return availability_match.group(1).strip()\n",
        "    return \"Specific\"\n",
        "\n",
        "def extract_origin(label_text):\n",
        "    if pd.isna(label_text):\n",
        "        return \"Unknown\"\n",
        "    origin_match = re.search(r'Asli: (.*?)($|;|,)', str(label_text))\n",
        "    if origin_match:\n",
        "        return origin_match.group(1).strip()\n",
        "    return \"Unknown\"\n",
        "\n",
        "# Data preprocessing function\n",
        "def preprocess_data(df_input):\n",
        "    data = df_input.copy()\n",
        "    # MODIFIED: Use 'label' column for extracting region, availability, and origin\n",
        "    if 'label' not in data.columns:\n",
        "        print(\"‚ùå Error: 'label' column not found. Cannot extract region, availability, and origin features.\")\n",
        "        # Add empty columns if 'label' is missing to prevent downstream errors, though they will be 'Unknown'\n",
        "        data['region'] = \"Unknown\"\n",
        "        data['availability'] = \"Unknown\"\n",
        "        data['origin'] = \"Unknown\"\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è Extracting region, availability, and origin from 'label' column.\")\n",
        "        data['region'] = data['label'].apply(extract_region)\n",
        "        data['availability'] = data['label'].apply(extract_availability)\n",
        "        data['origin'] = data['label'].apply(extract_origin)\n",
        "\n",
        "    # These are the nutritional columns the model expects for processing\n",
        "    numeric_cols = ['kalori (kkal)', 'energi (kj)', 'lemak (g)', 'lemak jenuh (g)',\n",
        "                    'lemak tak jenuh ganda (g)', 'lemak tak jenuh tunggal (g)',\n",
        "                    'kolesterol (mg)', 'protein (g)', 'karbohidrat (g)', 'serat (g)',\n",
        "                    'gula (g)', 'sodium (mg)', 'kalium (mg)']\n",
        "\n",
        "    print(\"\\nüîç Checking for expected numeric columns in the loaded data:\")\n",
        "    for col_check in numeric_cols:\n",
        "        if col_check not in data.columns:\n",
        "            print(f\"  ‚ùå Expected numeric column '{col_check}' NOT FOUND in loaded data. Model training might fail or be inaccurate.\")\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        if col in data.columns:\n",
        "            data[col] = data[col].astype(str).str.replace(',', '.', regex=False).str.strip()\n",
        "            data[col] = pd.to_numeric(data[col], errors='coerce')\n",
        "            data[col] = data[col].fillna(0)\n",
        "            print(f\"  ‚ÑπÔ∏è Column '{col}': NaN values filled with 0.\")\n",
        "\n",
        "    unique_regions = data['region'].dropna().unique()\n",
        "    for region_val in unique_regions: # Renamed region to region_val to avoid conflict\n",
        "        if pd.notna(region_val) and region_val != \"Unknown\":\n",
        "             sanitized_region_name = re.sub(r'[^A-Za-z0-9_]+', '', region_val.replace(\" \", \"_\"))\n",
        "             data[f'region_{sanitized_region_name}'] = (data['region'] == region_val).astype(int)\n",
        "\n",
        "    data['is_general'] = (data['availability'] == 'General').astype(int)\n",
        "    print(\"‚úÖ Data preprocessing functions defined and NaN in nutrition columns handled by filling with 0.\")\n",
        "    print(\"   Region, availability, and origin are now extracted from the 'label' column.\")\n",
        "    return data\n",
        "\n",
        "# Execute preprocessing\n",
        "if not df_raw.empty:\n",
        "    print(\"\\n‚öôÔ∏è Running data preprocessing...\")\n",
        "    processed_df = preprocess_data(df_raw)\n",
        "    print(\"‚úÖ Data preprocessing completed.\")\n",
        "    print(\"\\nüìã Sample of processed data (processed_df) including new 'origin' column:\")\n",
        "    # Display relevant columns including the new 'origin', 'region', 'availability'\n",
        "    display_cols = ['nama_makanan', 'label', 'region', 'availability', 'origin', 'kalori (kkal)', 'protein (g)']\n",
        "    # Filter out columns not present in processed_df to avoid KeyError\n",
        "    display_cols_existing = [col for col in display_cols if col in processed_df.columns]\n",
        "    print(processed_df[display_cols_existing].head())\n",
        "    print(\"\\n‚ÑπÔ∏è Info of processed data:\")\n",
        "    processed_df.info()\n",
        "\n",
        "    numeric_cols_check_after = ['kalori (kkal)', 'protein (g)', 'karbohidrat (g)', 'lemak (g)']\n",
        "    print(\"\\nüîç NaN check in key numeric columns AFTER preprocessing (should be 0 for these if column exists):\")\n",
        "    for col in numeric_cols_check_after:\n",
        "        if col in processed_df.columns:\n",
        "            print(f\"  NaNs in {col}: {processed_df[col].isnull().sum()}\")\n",
        "        else:\n",
        "            print(f\"  Column {col} not found for NaN check.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Raw data is empty. Skipping preprocessing.\")\n",
        "    processed_df = pd.DataFrame()"
      ],
      "metadata": {
        "id": "jk4p4LzCZqbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Define Nutritional Requirements and Balance Score Calculation (MODIFIED)\n",
        "def get_daily_nutritional_requirements():\n",
        "    return {\n",
        "        'kalori (kkal)': 2000, 'energi (kj)': 8368, # Added energi (kj) approx 2000 kcal * 4.184\n",
        "        'protein (g)': 56, 'karbohidrat (g)': 275,\n",
        "        'lemak (g)': 78, 'lemak jenuh (g)': 22, 'lemak tak jenuh ganda (g)': 18,\n",
        "        'lemak tak jenuh tunggal (g)': 25, 'kolesterol (mg)': 300, 'serat (g)': 28,\n",
        "        'gula (g)': 50, 'sodium (mg)': 2300, 'kalium (mg)': 3500\n",
        "    }\n",
        "\n",
        "def calculate_balance_score(current_nutrition, recommended_food_nutrition):\n",
        "    daily_req = get_daily_nutritional_requirements()\n",
        "    total_nutrition = {}\n",
        "    for nutrient in daily_req.keys():\n",
        "        current_val = current_nutrition.get(nutrient, 0) if isinstance(current_nutrition, dict) else 0\n",
        "        food_val = recommended_food_nutrition.get(nutrient, 0) if isinstance(recommended_food_nutrition, dict) else 0\n",
        "        total_nutrition[nutrient] = current_val + food_val\n",
        "\n",
        "    score = 0\n",
        "    for nutrient, req_val in daily_req.items(): # Renamed req to req_val\n",
        "        if nutrient in total_nutrition and req_val > 0:\n",
        "            val_total = total_nutrition[nutrient] # Renamed total_nutrition[nutrient] to val_total\n",
        "            if val_total > req_val:\n",
        "                score += ((val_total - req_val) / req_val) ** 2 * 1.5\n",
        "            else:\n",
        "                score += ((req_val - val_total) / req_val) ** 2\n",
        "    return score\n",
        "\n",
        "print(\"‚úÖ Nutritional requirement and balance score functions defined (energi (kj) added to requirements).\")"
      ],
      "metadata": {
        "id": "CwhX6dTMZsaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Define Model Building and Training Data Preparation Functions (MODIFIED)\n",
        "def build_recommendation_model(input_shape_val): # Renamed input_shape to input_shape_val\n",
        "    inputs = keras.Input(shape=input_shape_val) # Use input_shape_val\n",
        "    x = layers.Dense(64, activation='relu')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    encoded = layers.Dense(16, activation='relu', name='encoded')(x)\n",
        "    x = layers.Dense(32, activation='relu')(encoded)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    outputs = layers.Dense(input_shape_val[0], activation='linear')(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
        "    print(\"‚úÖ Recommendation model architecture defined.\")\n",
        "    return model\n",
        "\n",
        "def prepare_training_data(df_input): # Renamed df to df_input\n",
        "    # MODIFIED: Added 'energi (kj)' to nutritional_cols to match all numeric nutritional columns from the sheet\n",
        "    nutritional_cols = ['kalori (kkal)', 'energi (kj)', 'lemak (g)', 'lemak jenuh (g)',\n",
        "                        'lemak tak jenuh ganda (g)', 'lemak tak jenuh tunggal (g)',\n",
        "                        'kolesterol (mg)', 'protein (g)', 'karbohidrat (g)', 'serat (g)',\n",
        "                        'gula (g)', 'sodium (mg)', 'kalium (mg)']\n",
        "\n",
        "    existing_nutritional_cols = [col for col in nutritional_cols if col in df_input.columns]\n",
        "    if len(existing_nutritional_cols) < len(nutritional_cols):\n",
        "        print(f\"‚ö†Ô∏è Warning: Some nutritional columns are missing from input data. Using: {existing_nutritional_cols}\")\n",
        "    if not existing_nutritional_cols:\n",
        "        print(\"‚ùå Error: No nutritional columns found in the DataFrame. Cannot prepare training data.\")\n",
        "        return None, None, None, None, None, None, []\n",
        "\n",
        "    # Ensure data is numeric and handle NaNs before scaling\n",
        "    X_df = df_input[existing_nutritional_cols].copy()\n",
        "    for col in existing_nutritional_cols: # Ensure all selected columns are numeric\n",
        "        X_df[col] = pd.to_numeric(X_df[col], errors='coerce').fillna(X_df[col].median() if X_df[col].median() is not np.nan else 0)\n",
        "\n",
        "    X = X_df.values\n",
        "    if np.isnan(X).any():\n",
        "      print(\"‚ö†Ô∏è Warning: NaNs found in X features before scaling. This may cause issues. Attempting to fill with 0.\")\n",
        "      X = np.nan_to_num(X) # Replace NaNs with 0, a common strategy but check if appropriate\n",
        "\n",
        "    daily_req = get_daily_nutritional_requirements() # This function was modified in Cell 5\n",
        "    y = np.array([\n",
        "        [daily_req.get(col, 0) - (X[i, idx] if X[i, idx] is not None and not np.isnan(X[i, idx]) else 0)\n",
        "         for idx, col in enumerate(existing_nutritional_cols)]\n",
        "        for i in range(len(X))\n",
        "    ])\n",
        "    if np.isnan(y).any():\n",
        "      print(\"‚ö†Ô∏è Warning: NaNs found in y targets before scaling. Attempting to fill with 0.\")\n",
        "      y = np.nan_to_num(y)\n",
        "\n",
        "\n",
        "    X_scaler = StandardScaler()\n",
        "    y_scaler = StandardScaler()\n",
        "\n",
        "    X_scaled = X_scaler.fit_transform(X)\n",
        "    y_scaled = y_scaler.fit_transform(y)\n",
        "\n",
        "    if X_scaled.shape[0] < 2: # Need at least 2 samples for train_test_split\n",
        "        print(\"‚ùå Error: Not enough data points to split into training and testing sets.\")\n",
        "        return None, None, None, None, X_scaler, y_scaler, existing_nutritional_cols\n",
        "\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
        "    print(\"‚úÖ Training data preparation function defined (nutritional_cols now includes 'energi (kj)').\")\n",
        "    return X_train, X_test, y_train, y_test, X_scaler, y_scaler, existing_nutritional_cols\n",
        "\n",
        "# Prepare training data\n",
        "if not processed_df.empty:\n",
        "    print(\"\\n‚öôÔ∏è Preparing training data...\")\n",
        "    prepared_data = prepare_training_data(processed_df)\n",
        "    if prepared_data[0] is not None: # Check if X_train is not None\n",
        "        X_train, X_test, y_train, y_test, X_scaler, y_scaler, nutritional_cols_used = prepared_data\n",
        "        print(\"‚úÖ Training data prepared.\")\n",
        "        print(f\" Shapes: X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
        "        print(f\" Nutritional columns used for training: {nutritional_cols_used}\")\n",
        "\n",
        "        # Build the model\n",
        "        print(\"\\nüõ†Ô∏è Building the model...\")\n",
        "        # Ensure input_shape matches the number of features (nutritional_cols_used)\n",
        "        model = build_recommendation_model(input_shape_val=(X_train.shape[1],))\n",
        "        model.summary()\n",
        "    else:\n",
        "        print(\"‚ùå Training data preparation failed. Cannot build model.\")\n",
        "        # Initialize to prevent errors in later cells if they are run\n",
        "        X_train, X_test, y_train, y_test, X_scaler, y_scaler, nutritional_cols_used, model = [None]*8\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Processed data is empty. Skipping training data preparation and model building.\")\n",
        "    X_train, X_test, y_train, y_test, X_scaler, y_scaler, nutritional_cols_used, model = [None]*8"
      ],
      "metadata": {
        "id": "zpurgXNtZuzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Define Model Training and Visualization Functions\n",
        "def train_model_run(model_to_train, X_train_data, y_train_data, X_test_data, y_test_data, epochs=100, batch_size=32): # Renamed parameters\n",
        "    if model_to_train is None or X_train_data is None:\n",
        "        print(\"‚ùå Model or training data is not available. Skipping training.\")\n",
        "        return None, None\n",
        "    print(\"üèÉ‚Äç‚ôÇÔ∏è Starting model training...\")\n",
        "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
        "    reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "    history = model_to_train.fit(\n",
        "        X_train_data, y_train_data,\n",
        "        validation_data=(X_test_data, y_test_data),\n",
        "        epochs=epochs, batch_size=batch_size,\n",
        "        callbacks=[early_stopping, reduce_lr], verbose=1\n",
        "    )\n",
        "    print(\"‚úÖ Model training completed.\")\n",
        "    return model_to_train, history\n",
        "\n",
        "def visualize_training_history(history_data, title_prefix=\"\"): # Renamed history to history_data\n",
        "    if history_data is None:\n",
        "        print(\"‚ö†Ô∏è No training history to visualize.\")\n",
        "        return\n",
        "    print(f\"üìä Visualizing {title_prefix} training history...\")\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_data.history['loss'], label='Training Loss')\n",
        "    plt.plot(history_data.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{title_prefix} Loss')\n",
        "    plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_data.history['mae'], label='Training MAE')\n",
        "    plt.plot(history_data.history['val_mae'], label='Validation MAE')\n",
        "    plt.title(f'{title_prefix} Mean Absolute Error')\n",
        "    plt.xlabel('Epochs'); plt.ylabel('MAE'); plt.legend()\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# Train the model\n",
        "if model is not None and X_train is not None and y_train is not None and X_test is not None and y_test is not None :\n",
        "    trained_model, initial_history = train_model_run(model, X_train, y_train, X_test, y_test, epochs=100) # Reduced for Colab demo\n",
        "    visualize_training_history(initial_history, title_prefix=\"Initial Training\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping model training as model or data is not available.\")\n",
        "    trained_model, initial_history = None, None"
      ],
      "metadata": {
        "id": "5oi53NqCZyVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Define Model Fine-Tuning Function and Execute\n",
        "def fine_tune_model_run(model_to_fine_tune, X_train_data, y_train_data, X_test_data, y_test_data, epochs=50, batch_size=32, learning_rate=0.0001): # Renamed parameters\n",
        "    if model_to_fine_tune is None  or X_train_data is None:\n",
        "        print(\"‚ùå Model or training data is not available. Skipping fine-tuning.\")\n",
        "        return None, None\n",
        "    print(\"‚öôÔ∏è Starting model fine-tuning...\")\n",
        "    model_to_fine_tune.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])\n",
        "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    fine_tune_history = model_to_fine_tune.fit(\n",
        "        X_train_data, y_train_data,\n",
        "        validation_data=(X_test_data, y_test_data),\n",
        "        epochs=epochs, batch_size=batch_size,\n",
        "        callbacks=[early_stopping], verbose=1\n",
        "    )\n",
        "    print(\"‚úÖ Model fine-tuning completed.\")\n",
        "    return model_to_fine_tune, fine_tune_history\n",
        "\n",
        "# Fine-tune the model\n",
        "if trained_model is not None and X_train is not None :\n",
        "    fine_tuned_model, fine_tune_hist = fine_tune_model_run(trained_model, X_train, y_train, X_test, y_test, epochs=50) # Reduced for Colab\n",
        "    visualize_training_history(fine_tune_hist, title_prefix=\"Fine-Tuning\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping model fine-tuning as the initial trained model or data is not available.\")\n",
        "    fine_tuned_model = trained_model # Fallback to trained_model if fine-tuning skipped"
      ],
      "metadata": {
        "id": "DM-e3AFEZ0gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Define Recommendation Generation Function (MODIFIED)\n",
        "def generate_recommendations(model_rec, df_rec, current_foods_rec, user_location_rec, X_scaler_rec, y_scaler_rec, nutritional_cols_rec, n_recommendations=10):\n",
        "    if model_rec is None or df_rec.empty or X_scaler_rec is None or not nutritional_cols_rec:\n",
        "        print(\"‚ùå Cannot generate recommendations: Model, data, scaler, or nutritional columns missing.\")\n",
        "        return {'current_nutrition': {}, 'daily_requirements': get_daily_nutritional_requirements(), 'recommendations': [], 'potential_total_nutrition': {}}\n",
        "\n",
        "    current_nutrition = {col: 0 for col in nutritional_cols_rec}\n",
        "    for food_name, quantity in current_foods_rec:\n",
        "        food_data = df_rec[df_rec['nama_makanan'] == food_name]\n",
        "        if not food_data.empty:\n",
        "            for col in nutritional_cols_rec:\n",
        "                if col in food_data.columns and pd.notna(food_data.iloc[0][col]):\n",
        "                    current_nutrition[col] += food_data.iloc[0][col] * quantity\n",
        "\n",
        "    current_nutrition_array = np.array([[current_nutrition.get(col, 0) for col in nutritional_cols_rec]])\n",
        "    if not hasattr(X_scaler_rec, 'mean_') or X_scaler_rec.mean_ is None:\n",
        "        print(\"‚ö†Ô∏è X_scaler does not appear to be fitted. Recommendations may be inaccurate.\")\n",
        "        current_nutrition_scaled = current_nutrition_array\n",
        "    else:\n",
        "        current_nutrition_scaled = X_scaler_rec.transform(current_nutrition_array)\n",
        "\n",
        "    daily_req = get_daily_nutritional_requirements()\n",
        "    scores = []\n",
        "    for idx, row in df_rec.iterrows():\n",
        "        food_nutrition = {col: row[col] if col in row and pd.notna(row[col]) else 0 for col in nutritional_cols_rec}\n",
        "        if not any(food_nutrition.values()): continue\n",
        "\n",
        "        balance_score = calculate_balance_score(current_nutrition, food_nutrition)\n",
        "        location_bonus = 0\n",
        "        region_info = str(row.get('region', '')).lower()\n",
        "        availability_info = str(row.get('availability', '')).lower()\n",
        "        origin_info = str(row.get('origin', '')).lower() # Get origin info\n",
        "        is_general_flag = row.get('is_general', 0)\n",
        "\n",
        "        if user_location_rec and user_location_rec.lower() in region_info: location_bonus = -0.5\n",
        "        elif user_location_rec and user_location_rec.lower() in availability_info: location_bonus = -0.3\n",
        "        # Optional: Add bonus if user_location matches origin_info (can be adjusted or removed)\n",
        "        # if user_location_rec and user_location_rec.lower() in origin_info: location_bonus -= 0.1 \n",
        "        if is_general_flag == 1: location_bonus -= 0.2\n",
        "        adjusted_score = balance_score + location_bonus\n",
        "\n",
        "        scores.append({'food_name': row['nama_makanan'], 'score': adjusted_score,\n",
        "                       'region': row.get('region', 'N/A'), \n",
        "                       'origin': row.get('origin', 'N/A'), # MODIFIED: Include origin\n",
        "                       'is_general': is_general_flag,\n",
        "                       'nutrition_data': food_nutrition})\n",
        "    scores.sort(key=lambda x: x['score'])\n",
        "\n",
        "    recommendations = []\n",
        "    seen_foods = set()\n",
        "\n",
        "    if user_location_rec:\n",
        "        # Prioritize foods where region or availability matches user_location\n",
        "        local_foods = [f for f in scores if (user_location_rec.lower() in str(f.get('region', '')).lower() or \\\n",
        "                                            (f['food_name'] in df_rec['nama_makanan'].values and \\\n",
        "                                             user_location_rec.lower() in str(df_rec[df_rec['nama_makanan'] == f['food_name']]['availability'].iloc[0]).lower())) \\\n",
        "                                            and f['food_name'] not in seen_foods]\n",
        "        for food in local_foods:\n",
        "            if len(recommendations) < n_recommendations // 2 and food['food_name'] not in seen_foods:\n",
        "                recommendations.append(food); seen_foods.add(food['food_name'])\n",
        "\n",
        "    general_foods = [f for f in scores if f['is_general'] == 1 and f['food_name'] not in seen_foods]\n",
        "    for food in general_foods:\n",
        "        if len(recommendations) < n_recommendations and food['food_name'] not in seen_foods:\n",
        "            recommendations.append(food); seen_foods.add(food['food_name'])\n",
        "\n",
        "    other_foods = [f for f in scores if f['food_name'] not in seen_foods]\n",
        "    for food in other_foods:\n",
        "        if len(recommendations) < n_recommendations: recommendations.append(food); seen_foods.add(food['food_name'])\n",
        "\n",
        "    # MODIFIED: Include origin in formatted_recommendations\n",
        "    formatted_recommendations = [{'name': f['food_name'], 'score': f['score'], \n",
        "                                  'region': f['region'], 'origin': f['origin'],\n",
        "                                  'is_general': f['is_general'] == 1, \n",
        "                                  'nutrition': f['nutrition_data']}\n",
        "                                 for f in recommendations[:n_recommendations]]\n",
        "\n",
        "    potential_total = {col: current_nutrition.get(col,0) for col in nutritional_cols_rec}\n",
        "    for food in formatted_recommendations:\n",
        "        for col in nutritional_cols_rec: potential_total[col] += food['nutrition'].get(col, 0)\n",
        "\n",
        "    return {'current_nutrition': current_nutrition, 'daily_requirements': daily_req,\n",
        "            'recommendations': formatted_recommendations, 'potential_total_nutrition': potential_total}\n",
        "\n",
        "print(\"‚úÖ Recommendation generation function defined (includes origin).\")"
      ],
      "metadata": {
        "id": "Yb9j7PK7Z2TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Perform Inference Test (MODIFIED)\n",
        "print(\"üß™ Running inference test...\")\n",
        "if fine_tuned_model is not None and not processed_df.empty and X_scaler is not None and y_scaler is not None and nutritional_cols_used:\n",
        "    current_foods_example = []\n",
        "    if 'nama_makanan' in processed_df.columns and len(processed_df) > 0:\n",
        "        example_food_names = [\"Nasi Goreng\", \"Ayam Goreng\"]\n",
        "        food_names_in_df = processed_df['nama_makanan'].tolist()\n",
        "        for name in example_food_names:\n",
        "            if name in food_names_in_df:\n",
        "                current_foods_example.append((name, 1))\n",
        "        if not current_foods_example and len(food_names_in_df) >=1 :\n",
        "             current_foods_example.append((food_names_in_df[0],1))\n",
        "             if len(food_names_in_df) >=2:\n",
        "                current_foods_example.append((food_names_in_df[1],1))\n",
        "        elif not current_foods_example:\n",
        "             print(\"‚ö†Ô∏è Could not find example foods or any food in the dataset for inference test.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è 'nama_makanan' column missing or dataset empty, cannot select example foods for test.\")\n",
        "\n",
        "    user_location_example = \"Jakarta\"\n",
        "\n",
        "    if current_foods_example:\n",
        "        print(f\" Test current foods: {current_foods_example}, Location: {user_location_example}\")\n",
        "        test_recommendations = generate_recommendations(\n",
        "            fine_tuned_model, processed_df, current_foods_example, user_location_example,\n",
        "            X_scaler, y_scaler, nutritional_cols_used\n",
        "        )\n",
        "\n",
        "        print(\"\\n--- Inference Test Results ---\")\n",
        "        print(\"Current Nutrition:\")\n",
        "        for nutrient, value in test_recommendations['current_nutrition'].items(): print(f\"  {nutrient}: {value:.2f}\")\n",
        "        print(\"\\nRecommended Foods:\")\n",
        "        if test_recommendations['recommendations']:\n",
        "            for i, food in enumerate(test_recommendations['recommendations']):\n",
        "                # MODIFIED: Added origin to the print statement\n",
        "                print(f\"{i+1}. {food['name']} (Region: {food['region']}, Origin: {food['origin']}, General: {food['is_general']}) - Score: {food['score']:.4f}\")\n",
        "        else: print(\"  No recommendations generated for the test input.\")\n",
        "        print(\"\\nDaily Requirements:\")\n",
        "        for nutrient, value in test_recommendations['daily_requirements'].items(): print(f\"  {nutrient}: {value:.2f}\")\n",
        "        print(\"\\nPotential Total Nutrition (Current + All Recommendations):\")\n",
        "        for nutrient, value in test_recommendations['potential_total_nutrition'].items():\n",
        "            req_val = test_recommendations['daily_requirements'].get(nutrient, 0)\n",
        "            if req_val > 0: print(f\"  {nutrient}: {value:.2f} ({(value / req_val) * 100:.1f}% of daily requirement)\")\n",
        "            else: print(f\"  {nutrient}: {value:.2f}\")\n",
        "        print(\"--- End of Inference Test ---\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Skipping inference test as no current foods could be set up for the test.\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping inference test: Model, processed data, scalers, or nutritional columns not available.\")"
      ],
      "metadata": {
        "id": "9geqrMMAZ4KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Define Export, Zip, and Download Function & Execute (MODIFIED)\n",
        "def export_and_download_artifacts(model_to_export, df_processed_export, X_scaler_exp, y_scaler_exp, nutritional_cols_exp, base_dir='saved_model_food_recommendation'):\n",
        "    if model_to_export is None or X_scaler_exp is None or y_scaler_exp is None or not nutritional_cols_exp:\n",
        "        print(\"‚ùå Cannot export: Model, scalers, or nutritional columns missing.\")\n",
        "        return\n",
        "    if df_processed_export.empty:\n",
        "        print(\"‚ö†Ô∏è Processed DataFrame is empty. Skipping export of food labels.\")\n",
        "\n",
        "    print(f\"üöÄ Starting export to '{base_dir}'...\")\n",
        "    if os.path.exists(base_dir): shutil.rmtree(base_dir)\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "    # Save model as .keras\n",
        "    keras_path = os.path.join(base_dir, 'food_recommendation_model.keras')\n",
        "    model_to_export.save(keras_path)\n",
        "    print(f\"  üíæ Model saved in .keras format: {keras_path}\")\n",
        "\n",
        "    # Save model as .h5 (for TensorFlow.js converter or other compatibility)\n",
        "    h5_path = os.path.join(base_dir, 'food_recommendation_model.h5')\n",
        "    model_to_export.save(h5_path)\n",
        "    print(f\"  üíæ Model saved in .h5 format: {h5_path}\")\n",
        "\n",
        "    # Convert to TensorFlow.js\n",
        "    tfjs_model_dir = os.path.join(base_dir, 'tfjs_model')\n",
        "    os.makedirs(tfjs_model_dir, exist_ok=True)\n",
        "    print(f\"  üîÑ Attempting to convert {h5_path} to TensorFlow.js format in {tfjs_model_dir}...\")\n",
        "    try:\n",
        "        conversion_command = f\"tensorflowjs_converter --input_format=keras {h5_path} {tfjs_model_dir}\"\n",
        "        print(f\"    Executing: {conversion_command}\")\n",
        "        conversion_status = os.system(conversion_command)\n",
        "        if conversion_status == 0: print(f\"    ‚úÖ Model successfully converted to TensorFlow.js.\")\n",
        "        else: print(f\"    ‚ùå Error during TensorFlow.js conversion. Status code: {conversion_status}\")\n",
        "    except Exception as e: print(f\"    ‚ùå Exception during TensorFlow.js conversion: {e}\")\n",
        "\n",
        "    # Save metadata (nutritional columns and daily requirements) and scalers\n",
        "    metadata_for_json = {'nutritional_columns': nutritional_cols_exp, 'daily_requirements': get_daily_nutritional_requirements()}\n",
        "    with open(os.path.join(base_dir, 'metadata.json'), 'w') as f: json.dump(metadata_for_json, f, indent=4)\n",
        "    joblib.dump(X_scaler_exp, os.path.join(base_dir, 'X_scaler.pkl'))\n",
        "    joblib.dump(y_scaler_exp, os.path.join(base_dir, 'y_scaler.pkl'))\n",
        "    print(f\"  üíæ Metadata and scalers saved in {base_dir}\")\n",
        "\n",
        "    # MODIFIED: Export food labels (nama_makanan, label, region, origin, availability) to JSON\n",
        "    if not df_processed_export.empty:\n",
        "        food_labels_path = os.path.join(base_dir, 'food_labels_and_origins.json') # Changed to .json\n",
        "        label_columns_to_export = ['nama_makanan', 'label', 'region', 'availability', 'origin']\n",
        "        existing_label_columns = [col for col in label_columns_to_export if col in df_processed_export.columns]\n",
        "        if existing_label_columns:\n",
        "            # Convert selected columns to a list of dictionaries (records) and then to JSON\n",
        "            labels_data = df_processed_export[existing_label_columns].to_dict(orient='records')\n",
        "            with open(food_labels_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(labels_data, f, ensure_ascii=False, indent=4)\n",
        "            print(f\"  üíæ Food labels and origins saved to: {food_labels_path} (JSON format)\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Could not export food labels as relevant columns are missing from processed_df.\")\n",
        "\n",
        "    # Zip the directory\n",
        "    zip_filename = f\"{base_dir}.zip\"\n",
        "    shutil.make_archive(base_dir, 'zip', '.', base_dir) # Zip contents of base_dir\n",
        "    print(f\"  üì¶ Directory '{base_dir}' zipped to '{zip_filename}'\")\n",
        "\n",
        "    # Download the zip file\n",
        "    files.download(zip_filename)\n",
        "    print(f\"  üì• Download of '{zip_filename}' initiated.\")\n",
        "    print(\"‚úÖ Export, zip, and download process completed.\")\n",
        "\n",
        "# Execute export\n",
        "if fine_tuned_model is not None and not processed_df.empty and X_scaler is not None and y_scaler is not None and nutritional_cols_used:\n",
        "    export_and_download_artifacts(fine_tuned_model, processed_df, X_scaler, y_scaler, nutritional_cols_used)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Skipping export: Fine-tuned model, processed data, scalers, or nutritional columns not available.\")\n",
        "\n",
        "print(\"\\nüéâüéâüéâ Food Recommendation Model Pipeline in Colab (Script-Style) Finished! üéâüéâüéâ\")"
      ],
      "metadata": {
        "id": "Ps_wOegXZ6D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bQ3B4JGw_m-F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
